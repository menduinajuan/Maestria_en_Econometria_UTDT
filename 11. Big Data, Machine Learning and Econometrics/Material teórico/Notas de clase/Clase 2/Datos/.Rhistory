# Primero creo/defino la función RE en la memoria de R:
RE = function(theta){ -log(theta^2+1) + theta^2}
RE(1) # Puedo evaluar la función RE donde quiera.
# Podemos graficar las funciones con el comando plot:
par(mfrow = c(1,2), mar = c(4.4,4.4,1,1))  # Parámetros gráficos
# Si hacemos la cuenta, es fácil ver que theta* = 0. Usemos el
# gradiente descendiente para aproximar esta solución numéricamente:
RE_prima = function(theta){ -(2*theta)/(theta^2+1) + 2*theta}
abline(h = 0, col = 'gray', lty = 4)
plot(RE, from = -5, to = 5, xlab = expression(theta),lwd = 2)
# Podemos graficar las funciones con el comando plot:
par(mfrow = c(1,2), mar = c(4.4,4.4,1,1))  # Parámetros gráficos
plot(RE, from = -5, to = 5, xlab = expression(theta),lwd = 2)
RE_prima = function(theta){ -(2*theta)/(theta^2+1) + 2*theta}
plot(RE_prima, from = -5, to = 5, xlab = expression(theta),lwd = 2)
abline(h = 0, col = 'gray', lty = 4)
theta = -4 # Valor inicial ó lo que llamamos theta_0 (en las slides)
points(theta,0, col='red', pch = 20)
lambda = 0.1
theta = theta - lambda*RE_prima(theta)
points(theta,0, col='blue', pch = 20) # Primera actualización.
theta = theta - lambda*RE_prima(theta)
print(theta)
points(theta,0, col='green', pch = 20) # Segunda actualización.
theta = theta - lambda*RE_prima(theta)
print(theta)
points(theta,0, col='brown', pch = 20) # Tercera actualización.
theta = -4
for (i in 1:10000) {
theta = theta - lambda*RE_prima(theta)
print(i)
}
theta = -4
for (i in 1:10000) {
theta = theta - lambda*RE_prima(theta)
print(i)
}
print(theta)
### Simulando datos de un modelo de regresión con 1 covariables:
set.seed(1)
x = rnorm(n = 100, mean = 0, sd = 1); x = sort(x)
y = 1 + 2*x + rnorm(100,0,1)  # Verdaderos parámetros (beta_0 = 1 y beta_1 = 2).
x[101]= 7; y[101] = -5 # Outlier
x11()
plot(x, y, pch = 20, ylim = c(-5,7), xlim = c(-5,7))
points(x[101], y[101] , col = 'blue', lwd = 4)
summary(lm(y~x)) # hat.beta_1.ols = 0.87 (sensiblemente menor a 2 = beta_1, porqué?)
points(x,0.95+ 0.92*x, type = 'l', col = 'blue', lty = 3)
points(x,1+ 2*x, type = 'l', col = 'gray', lty = 3) # Verdadera función de regresión en la población.
#------   Newton-Raphson   ----------------------------------#
g = RE_prima
g_prima = function(theta){ # Derivada segunda de RE
2*(theta^2 - 1)/ (theta^2 + 1)^2 + 2
}
plot(RE, from = -5, to = 5, xlab = expression(theta), lwd = 2, ylab ='g')
plot(RE_prima, from = -5, to = 5, xlab = expression(theta), ylab ="g'" ,lwd = 2)
abline(h = 0, col = 'gray', lty = 4)
theta = -4
points(theta,0, col='red', pch = 20)
theta = theta - g(theta)/g_prima(theta)
print(theta) # Primer update
points(theta,0, col='blue', pch = 20)
theta = theta - g(theta)/g_prima(theta)
print(theta) # Segundo update
points(theta,0, col='green', pch = 20)
theta = theta - g(theta)/g_prima(theta)
print(theta) # Tercer update
points(theta,0, col='brown', pch = 20)
for (i in 1:10) {
theta = theta - g(theta)/g_prima(theta)
}
print(theta)
################# kNN en R: https://cran.r-project.org/web/packages/FNN/FNN.pdf
# install.packages('FNN');
library(FNN)
################# kNN en R: https://cran.r-project.org/web/packages/FNN/FNN.pdf
install.packages('FNN');
library(FNN)
library(FNN)
f <- function(X){sin(2*X) + sqrt(X)}
X = seq(0,4*pi,length.out = 100)
set.seed(123) # Fijamos la semilla para que todos tengamos el mismo data set.
Y =  f(X) + rnorm(100)
par(mar=c(4,4,1,1))
plot(X,f(X), type = 'l',ylab='Y',
ylim = c(-3,6),bty='n', lwd = 3, lty = 1)
points(X,Y, pch = 20)
################################################################################
head(knn.index(X, k = 2), 3)  # Para cada x del data set de train sus 2 vecinos cercanos.
install.packages('FNN');
