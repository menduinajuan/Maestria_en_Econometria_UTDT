sep=',', header=T, dec='.', na.strings = "NA")
# Una versión refinada de 'pairs' con temperatura, humedad, windspeed y cnt.
dev.off()
pairs.panels(datos[,c(11:13,16)], cex = 3,  # variables a analizar: atemp hum windspeed cnt
method = "pearson",    # Método para computar la correlación
hist.col = "#00AFBB",
smooth = TRUE, # Fitea un modelo no paramétrico de regresión entre pares de var
density = TRUE,  # Estima densidades para cada una de las variables
ellipses = FALSE # Ilustra la correlación entre las variables utilizando elipses
)
# Una versión refinada de 'pairs' con temperatura, humedad, windspeed y cnt.
dev.off()
### Sampleando de modelos estadísticos concretos
rm(list = ls()) # Borramos todo lo que R tenga en memoria
### Loop "for": # Super utilizado en el curso!!!!!
for(i in 1:5){ print(i) }
for(j in 1:3){ # Un loop for adentro de otro for
print(i+j) }
for(i in 1:2){
for(j in 1:3){ # Un loop for adentro de otro for
print(i+j) }
}
# Ejemplo 1: TCL
realiz.med.muestral = c()
for(i in 1:1000){
datos                  = rexp(20, rate = 0.1) # E(X) = 10
realiz.med.muestral[i] = mean(datos)
}
hist(realiz.med.muestral, main='TCL')
# Ejemplo 2: Crear un vector con Fib = \{1,1,2,3,5,8...\}        ##
Fibonacci = c() # Creamos un vector 'vacío' de dimensión arbitraria.         ##
Fibonacci[1] = 1; # Asignamos al primer elemento del vector el valor 1.      ##
Fibonacci[2] = 1; # Asignamos al segundo elemento del vector el valor 1.     ##
for(i in 3:48){Fibonacci[i]= Fibonacci[i-1]+Fibonacci[i-2]}                  ##
# Para i desde 3 hasta 48, el elemento que ocupa la posición i es igual      ##
# a  la suma de los DOS elementos inmediatamente anteriores en dicho vector. ##
print(Fibonacci)                                                             ##
###############################################################################
# EJERCICIO EN CLASE: lee el fichero de datos en R datos.RData               ##
setwd('G:/Mi unidad/JM/Facultad de Ciencias Económicas (FCE)/Maestría en Econometría/11. Big Data, Machine Learning and Econometrics/Material teórico/Notas de clase/Clase 1/Datos')##
load('datos.RData')                                                          ##
dim(datos)                                                                   ##
##
##
pairs(datos)                                                                 ##
##
reg = lm(datos[,1] ~ datos[,2], data = datos)                                ##
summary(reg)                                                                 ##
##
reg$coefficients # ¿que información extraemos del modelo?                    ##
pendientes <- numeric(ncol(datos) - 1)  # Vector para almacenar las pendientes
for (i in 2:ncol(datos)) {
model <- lm(datos[, 1] ~ datos[, i], data = datos)
pendientes[i - 1] <- model$coefficients[2]
}
print(pendientes)
indice_max_pendiente <- which.max(pendientes)
modelo_max_pendiente <- colnames(datos)[indice_max_pendiente + 1]  # Ajustar índice
cat("El modelo con mayor pendiente es el correspondiente a la variable:", modelo_max_pendiente)
# Loop "while":
i = 1
while(i<=5){print(i);
i = i+1}
### Condicional "if{} else {}"
p = 3; q = 2
if(p<q){ print('p es menor que q')} else {print('p es mayor o igual que q')}
# En algunos casos, puede resultar de utilidad combinar ciclos con condicionales:
for(i in 1:10){
if(i<5){print("i es menor que 5")} else if(i==5){ print("i es = 5")
} else {print("i es mayor que 5")}
}
## Condicional ifelse (sobre vectores o matrices)
x = c(-1,2,8,21,-2,11,9,-1,7)
print(x)
x = ifelse (x < 0 | x > 10,0,x)
print(x)
M = matrix(c(-1,2,8,21,-2,11,9,-1,7),ncol=3)
print(M)
ifelse (M < 0 | M > 10,0,M)
mifuncion = function(x){
return(x^2 - 3*x + 1)
}
mifuncion
mifuncion(1);
mifuncion(x=c(1:4)) # La puedo evaluar en un conjunto de argumentos.
plot(mifuncion, from=-1, to =5)
mifuncion2 = function(x,a,b,c){
return(a*x^2 - b*x + c)
}
x = seq(-1,5, by = 0.5)
points(x,mifuncion2(x,a=0.5,b=1,c=2), type='l', col = 'red')
### Missing data:
datos = data.frame(x1 = c(1:5), x2 = c(5:3,NA,1))
datos
which(is.na(datos)==T, arr.ind = T)
datos2 = na.omit(datos)
datos2
datos2 = as.matrix(datos2)
class(datos2)
## bucle for 'anidado'
#@ En algún momento podríamos llegar a necesitar utilizar un for adentro de
#@ otro for (cuando cross--validemos más de un parámetro). Veamos un ejemplo:
M = matrix(0,ncol=3,nrow=3)
for(i in 1:3){
for(j in 1:3){  M[i,j]= i+j }    }
print(M)
### Grillas:
#@ Para evitar tener que utilizar dos (o más) 'for', en algunas ocasiones
#@ podemos crear 'grillas' con valores para ciertos parámetros sensibles.
param1 = c(1,2,3)
param2 = c(9,8)
grilla = expand.grid(param1, param2)
grilla
### Data frames (formato habitual de los datos que levantamos con R)
datos = read.table("G:/Mi unidad/JM/Facultad de Ciencias Económicas (FCE)/Maestría en Econometría/11. Big Data, Machine Learning and Econometrics/Material teórico/Notas de clase/Clase 1/Datos/bikes.csv",
sep=',', header=T, dec='.', na.strings = "NA")
View(datos)
View(datos)
###############################################################################
# EJERCICIO EN CLASE: lee el fichero de datos en R datos.RData               ##
setwd('G:/Mi unidad/JM/Facultad de Ciencias Económicas (FCE)/Maestría en Econometría/11. Big Data, Machine Learning and Econometrics/Material teórico/Notas de clase/Clase 1/Datos')##
load('datos.RData')                                                          ##
dim(datos)                                                                   ##
##
reg = lm(datos[,1] ~ datos[,2], data = datos)                                ##
##
##
pairs(datos)                                                                 ##
##
reg = lm(datos[,1] ~ datos[,2], data = datos)                                ##
summary(reg)                                                                 ##
##
reg$coefficients # ¿que información extraemos del modelo?                    ##
pendientes <- numeric(ncol(datos) - 1)
for (i in 2:ncol(datos)) {
model <- lm(datos[, 1] ~ datos[, i], data = datos)
pendientes[i - 1] <- model$coefficients[2]
}
cls
clear
### Creando vectores
vector = c(1,3,5,7,9,11)
vector
length(vector) # Dimensión del vector.
sum(vector)
max(vector)
min(vector)
## Operando 'punto a punto' con vectores
2*vector + 1
vector^2
log(vector)
exp(vector)
dias = c('lunes', 'martes', 'miercoles', 'jueves', 'viernes')
dias
2*dias + 1 # Que va a ocurrir?
## Algunas funciones útiles:
rep('R',3)        # Repetir elemento una cantidad arbitraria de veces.
seq(0,1, by=0.1)  # Generar secuencias de longitud arbitraria.
### Acceder a los elementos de un vector '[]'
vector
vector[3]
vector[-3]
### Buscando "elementos" adentro de un vector:
set.seed(1)
z = runif(10) # Genero 10 números "aleatorios" en el intervalo [0,1]
z
which(z>0.1 & z<=0.3)    # Posiciones de los elementos de z que son están en (0.1,0.3).
z[which(z>0.1 & z<=0.3)] # Elementos del vector z que cumplen la condición anterior.
which( (z>0.1 & z<=0.3) | (z>0.9) ) # Puedo combinar condiconales como quiera.
which(z == max(z)) # print(z)
which(z == min(z)) # print(z)
# Concatenando vectores:
x = 1:3
y = 9:11
z = c(y,x,-1)
print(z) # Primero y, después x y luego un '-1'.
### Estructuras matriciales en R
print(vector)
matriz = matrix(vector,ncol=2, nrow = 3)
matriz
matriz = matrix(vector,ncol=2, byrow = TRUE )
matriz
### Dimensiones de una matriz
dim(matriz)
# Operaciones 'punto a punto' con los elementos de una matriz
2*matriz + 1
A = matrix(1:9,ncol=3)
A
A + 1
2*A
log(A) #idem para exp(A), sin(A), etc etc etc
A
A*c(1,0,1)
A%*%c(1,0,1) # vector
B = matrix(1:9,ncol=3, byrow = T)
B
A*B # operación punto a punto
A%*%B # AB
C = matrix(runif(9),ncol=3, byrow = T)
C
solve(C) # C^{-1}
# Concatenando matrices
rbind(A,B) # 6x3
cbind(A,B) # 3x6
# Darle nombres (renombrar) las columnas y/o las filas:
colnames(A) <- c('y','x1','x2')
rownames(A) <- c('id1','id2','id3')
print(A)
### Acceder a los elementos de una matriz '[filas,columnas]'
print(A)
A[1,2]     # el elemento de la posición 1,1.
A[1:2,2:3] # los elementos de las posición 1 a 2, 2 a 3.
which(A==5 | A>7, arr.ind = T) # | = 'o' en cambio & = 'y'
## Aplicar funciones sobre filas o columnas ('apply'):
print(A)
apply(A,1,sum) # Devuelve la suma por filas de A
apply(A,2,min) # Devuelve los mínimos por columnas de A
### Data frames (formato habitual de los datos que levantamos con R)
datos = read.table("G:/Mi unidad/JM/Facultad de Ciencias Económicas (FCE)/Maestría en Econometría/11. Big Data, Machine Learning and Econometrics/Material teórico/Notas de clase/Clase 1/Datos/bikes.csv",
sep=',', header=T, dec='.', na.strings = "NA")
### Data frames (formato habitual de los datos que levantamos con R)
datos = read.table("G:/Mi unidad/JM/Facultad de Ciencias Económicas (FCE)/Maestría en Econometría/11. Big Data, Machine Learning and Econometrics/Material teórico/Notas de clase/Clase 1/Datos/bikes.csv",
sep=',', header=T, dec='.', na.strings = "NA")
class(datos)
str(datos) # Las columnas de esta matriz pueden contener variables de distinto tipo.
head(datos,2) # Visualizar las primeras filas del data set.
datos$cnt # accedo a la columna cuyo nombre es "cnt"
attach(datos) # attach de los nombres de las variables en memoria.
cnt # como hice un attach de datos, al llamar a "cnt" R entiende datos$cnt
summary(datos) # Algunas estadísticas básicas de lo que hay en cada columna.
table(weathersit, holiday) # Tabla de frecuencias para variables categóricas.
table(holiday, cut(cnt,breaks=c(1000,2500,5000,10000),dig.lab = 4)) # Tabla de frecuencias para variables mixtas.
### Sampleando de modelos estadísticos concretos
rm(list = ls()) # Borramos todo lo que R tenga en memoria
set.seed(1)
runif(1) # help(runif) o ?runif  <- Ayuda para comprender mejor el comando y sus argumentos.
rnorm(5,mean=0,sd=1)
rexp(5, rate = 2)
rpois(5, lambda = 3)
rbeta(5, shape1 = 1, shape2=2)
## Análisis estadístico descriptivo:
# set.seed(1)
x = rnorm(100,mean=0,sd=1)
head(x, 3)                # Observamos primeras filas de un vector / matriz.
mean(x)
var(x)
quantile(x, c(0.1,0.5,0.75))
summary(x)
hist(x)
hist(x, main='Histograma',
xlab = 'Mis datos',
ylab = 'Frecuencia Relativa',
col = 'blue',
probability = TRUE,
breaks = c(-6,-4,-3,-2,-1,0,1,2,3,4,6))
boxplot(x, col='red', xlab='eje-X', ylab = 'eje-Y')
### Exportar gráficas fuera de R:
#png('exportandomigrafico.png', width = 800, height = 450)
par(mfrow=c(1,2)) # juntar gráficas en una misma ventana:
hist(x)
boxplot(x)
### Scatterplot matrix (análisis gráfico conjunto de varias variables numéricas)
pairs(datos[,c(11:13,16)], pch = 20, col = 'gray')
datos = read.table("G:/Mi unidad/JM/Facultad de Ciencias Económicas (FCE)/Maestría en Econometría/11. Big Data, Machine Learning and Econometrics/Material teórico/Notas de clase/Clase 1/Datos/bikes.csv",
sep=',', header=T, dec='.', na.strings = "NA")
pairs(datos[,c(11:13,16)], pch = 20, col = 'gray')
rm(list = ls())
set.seed(1)
n <- 100
X <- runif(n, min = 3, max = 5)
eps <- rnorm(n, mean = 0, sd = 1)
Y <- -1 + 0.5 * X + eps
plot(X, Y, main = "Gráfico de Y vs X", xlab = "X", ylab = "Y", pch = 19, col = "blue")
mod.lin = lm(Y~X)
summary(mod.lin)
mod.lin$residuals
mod.lin$coefficients
attributes(mod.lin)
# Paquetes de R: https://cran.r-project.org/web/packages/available_packages_by_name.html
install.packages('psych') # Instalamos el paquete desde la consola (1 vez)
library(psych)
# Una versión refinada de 'pairs' con temperatura, humedad, windspeed y cnt.
dev.off()
pairs.panels(datos[,c(11:13,16)], cex = 3,  # variables a analizar: atemp hum windspeed cnt
method = "pearson",    # Método para computar la correlación
hist.col = "#00AFBB",
smooth = TRUE, # Fitea un modelo no paramétrico de regresión entre pares de var
density = TRUE,  # Estima densidades para cada una de las variables
ellipses = FALSE # Ilustra la correlación entre las variables utilizando elipses
)
datos = read.table("G:/Mi unidad/JM/Facultad de Ciencias Económicas (FCE)/Maestría en Econometría/11. Big Data, Machine Learning and Econometrics/Material teórico/Notas de clase/Clase 1/Datos/bikes.csv",
sep=',', header=T, dec='.', na.strings = "NA")
pairs.panels(datos[,c(11:13,16)], cex = 3,  # variables a analizar: atemp hum windspeed cnt
method = "pearson",    # Método para computar la correlación
hist.col = "#00AFBB",
smooth = TRUE, # Fitea un modelo no paramétrico de regresión entre pares de var
density = TRUE,  # Estima densidades para cada una de las variables
ellipses = FALSE # Ilustra la correlación entre las variables utilizando elipses
)
View(mod.lin)
### Loop "for": # Super utilizado en el curso!!!!!
for(i in 1:5){ print(i) }
for(i in 1:2){
for(j in 1:3){ # Un loop for adentro de otro for
print(i+j) }
}
# Ejemplo 1: TCL
realiz.med.muestral = c()
for(i in 1:1000){
datos                  = rexp(20, rate = 0.1) # E(X) = 10
realiz.med.muestral[i] = mean(datos)
}
hist(realiz.med.muestral, main='TCL')
# Ejemplo 2: Crear un vector con Fib = \{1,1,2,3,5,8...\}        ##
Fibonacci = c() # Creamos un vector 'vacío' de dimensión arbitraria.         ##
Fibonacci[1] = 1; # Asignamos al primer elemento del vector el valor 1.      ##
Fibonacci[2] = 1; # Asignamos al segundo elemento del vector el valor 1.     ##
for(i in 3:48){Fibonacci[i]= Fibonacci[i-1]+Fibonacci[i-2]}                  ##
# Para i desde 3 hasta 48, el elemento que ocupa la posición i es igual      ##
# a  la suma de los DOS elementos inmediatamente anteriores en dicho vector. ##
print(Fibonacci)                                                             ##
###############################################################################
# EJERCICIO EN CLASE: lee el fichero de datos en R datos.RData               ##
setwd('G:/Mi unidad/JM/Facultad de Ciencias Económicas (FCE)/Maestría en Econometría/11. Big Data, Machine Learning and Econometrics/Material teórico/Notas de clase/Clase 1/Datos')##
load('datos.RData')                                                          ##
dim(datos)                                                                   ##
##
reg = lm(datos[,1] ~ datos[,2], data = datos)                                ##
summary(reg)                                                                 ##
##
reg$coefficients # ¿que información extraemos del modelo?                    ##
pendientes <- numeric(ncol(datos) - 1)
for (i in 2:ncol(datos)) {
model <- lm(datos[, 1] ~ datos[, i], data = datos)
pendientes[i - 1] <- model$coefficients[2]
}
print(pendientes)
indice_max_pendiente <- which.max(pendientes)
modelo_max_pendiente <- colnames(datos)[indice_max_pendiente + 1]
cat("El modelo con mayor pendiente es el correspondiente a la variable:", modelo_max_pendiente)
View(datos)
View(datos)
# Loop "while":
i = 1
while(i<=5){print(i);
i = i+1}
### Condicional "if{} else {}"
p = 3; q = 2
if(p<q){ print('p es menor que q')} else {print('p es mayor o igual que q')}
# En algunos casos, puede resultar de utilidad combinar ciclos con condicionales:
for(i in 1:10){
if(i<5){print("i es menor que 5")} else if(i==5){ print("i es = 5")
} else {print("i es mayor que 5")}
}
## Condicional ifelse (sobre vectores o matrices)
x = c(-1,2,8,21,-2,11,9,-1,7)
print(x)
x = ifelse (x < 0 | x > 10,0,x)
print(x)
M = matrix(c(-1,2,8,21,-2,11,9,-1,7),ncol=3)
print(M)
ifelse (M < 0 | M > 10,0,M)
mifuncion = function(x){
return(x^2 - 3*x + 1)
}
mifuncion
mifuncion(1);
mifuncion(x=c(1:4)) # La puedo evaluar en un conjunto de argumentos.
plot(mifuncion, from=-1, to =5)
mifuncion2 = function(x,a,b,c){
return(a*x^2 - b*x + c)
}
x = seq(-1,5, by = 0.5)
points(x,mifuncion2(x,a=0.5,b=1,c=2), type='l', col = 'red')
# Primero creo/defino la función RE en la memoria de R:
RE = function(theta){ -log(theta^2+1) + theta^2}
RE(1) # Puedo evaluar la función RE donde quiera.
# Podemos graficar las funciones con el comando plot:
par(mfrow = c(1,2), mar = c(4.4,4.4,1,1))  # Parámetros gráficos
plot(RE, from = -5, to = 5, xlab = expression(theta),lwd = 2)
RE_prima = function(theta){ -(2*theta)/(theta^2+1) + 2*theta}
plot(RE_prima, from = -5, to = 5, xlab = expression(theta),lwd = 2)
abline(h = 0, col = 'gray', lty = 4)
theta = -4 # Valor inicial ó lo que llamamos theta_0 (en las slides)
points(theta,0, col='red', pch = 20)
lambda = 0.1
theta = theta - lambda*RE_prima(theta)
print(theta)
points(theta,0, col='blue', pch = 20) # Primera actualización.
theta = theta - lambda*RE_prima(theta)
print(theta)
points(theta,0, col='green', pch = 20) # Segunda actualización.
theta = theta - lambda*RE_prima(theta)
print(theta)
points(theta,0, col='brown', pch = 20) # Tercera actualización.
#### Minimizamos la función RE(theta) = -log(theta^2+1) + theta^2
#------   Gradiente descendiente  ----------------------------------#
# Primero creo/defino la función RE en la memoria de R:
RE = function(theta){ -log(theta^2+1) + theta^2}
RE(1) # Puedo evaluar la función RE donde quiera.
# Podemos graficar las funciones con el comando plot:
par(mfrow = c(1,2), mar = c(4.4,4.4,1,1))  # Parámetros gráficos
plot(RE, from = -5, to = 5, xlab = expression(theta),lwd = 2)
# Si hacemos la cuenta, es fácil ver que theta* = 0. Usemos el
# gradiente descendiente para aproximar esta solución numéricamente:
RE_prima = function(theta){ -(2*theta)/(theta^2+1) + 2*theta}
plot(RE_prima, from = -5, to = 5, xlab = expression(theta),lwd = 2)
abline(h = 0, col = 'gray', lty = 4)
theta = -4 # Valor inicial ó lo que llamamos theta_0 (en las slides)
points(theta,0, col='red', pch = 20)
lambda = 0.1
theta = theta - lambda*RE_prima(theta)
print(theta)
points(theta,0, col='blue', pch = 20) # Primera actualización.
theta = theta - lambda*RE_prima(theta)
print(theta)
points(theta,0, col='green', pch = 20) # Segunda actualización.
theta = -4
for (i in 1:10000) {
theta = theta - lambda*RE_prima(theta)
print(i)
}
print(theta)
#------   Newton-Raphson   ----------------------------------#
g = RE_prima
g_prima = function(theta){ # Derivada segunda de RE
2*(theta^2 - 1)/ (theta^2 + 1)^2 + 2
}
plot(RE, from = -5, to = 5, xlab = expression(theta), lwd = 2, ylab ='g')
plot(RE_prima, from = -5, to = 5, xlab = expression(theta), ylab ="g'" ,lwd = 2)
abline(h = 0, col = 'gray', lty = 4)
theta = -4
points(theta,0, col='red', pch = 20)
theta = theta - g(theta)/g_prima(theta)
print(theta) # Primer update
points(theta,0, col='blue', pch = 20)
theta = theta - g(theta)/g_prima(theta)
print(theta) # Segundo update
points(theta,0, col='green', pch = 20)
theta = theta - g(theta)/g_prima(theta)
print(theta) # Tercer update
points(theta,0, col='brown', pch = 20)
for (i in 1:10) {
theta = theta - g(theta)/g_prima(theta)
}
print(theta)
### Simulando datos de un modelo de regresión con 1 covariables:
set.seed(1)
x = rnorm(n = 100, mean = 0, sd = 1); x = sort(x)
y = 1 + 2*x + rnorm(100,0,1)  # Verdaderos parámetros (beta_0 = 1 y beta_1 = 2).
x[101]= 7; y[101] = -5 # Outlier
x11()
plot(x, y, pch = 20, ylim = c(-5,7), xlim = c(-5,7))
summary(lm(y~x)) # hat.beta_1.ols = 0.87 (sensiblemente menor a 2 = beta_1, porqué?)
points(x,0.95+ 0.92*x, type = 'l', col = 'blue', lty = 3)
points(x,1+ 2*x, type = 'l', col = 'gray', lty = 3) # Verdadera función de regresión en la población.
################# kNN en R: https://cran.r-project.org/web/packages/FNN/FNN.pdf
# install.packages('FNN');
library(FNN)
##### 1) Simulando los datos de entrenamiento:
f <- function(X){sin(2*X) + sqrt(X)}
X = seq(0,4*pi,length.out = 100)
set.seed(123) # Fijamos la semilla para que todos tengamos el mismo data set.
Y =  f(X) + rnorm(100)
par(mar=c(4,4,1,1))
plot(X,f(X), type = 'l',ylab='Y',
ylim = c(-3,6),bty='n', lwd = 3, lty = 1)
points(X,Y, pch = 20)
################################################################################
head(knn.index(X, k = 2), 3)  # Para cada x del data set de train sus 2 vecinos cercanos.
head(knn.dist(X, k = 2 ), 3 ) # las respectivas distancias.
## Regresión con k--vecinos:
kNN.reg = knn.reg(train = X , test = NULL, y = Y, k = 10)
head(kNN.reg$pred, 3)# Predicciones (valores de Y predichos para las primeras 3 obs).
head(kNN.reg$residuals, 3)# Distancia entre predicciones y valores observados.
sum(kNN.reg$residuals^2) # Suma de cuadrados residuales
1- sum(kNN.reg$residuals^2) / sum( (Y-mean(Y))^2 ) # R^2 = 1-SCR/SCT
kNN.reg # PRESS = suma de cuadrados residuales.
points(X, kNN.reg$pred, type='l', col = 'red', lwd = 2, lty = 2)
kNN.reg = knn.reg(train = X , test = NULL, y = Y, k = 50)
kNN.reg # PRESS = Suma de cuadrados
points(X, kNN.reg$pred, type='l', col = 'blue', lwd = 2, lty = 2)
kNN.reg = knn.reg(train = X , test = NULL, y = Y, k = 1)
points(X, kNN.reg$pred, type='l', col = 'green', lwd = 2, lty = 2)
################# kNN en R: https://cran.r-project.org/web/packages/FNN/FNN.pdf
# install.packages('FNN');
library(FNN)
##### 1) Simulando datos de entrenamiento:
library(MASS)
set.seed(123) # Fijamos la semilla para que todos tengamos el mismo data set.
X =  rbind(mvrnorm(50,c(-1,-1),diag(2)),mvrnorm(50,c(1,1),diag(2)))
dim(X)
Y =  factor(c(rep('A',50),rep('B',50) ))
par(mar=c(4,4,1,1))
plot(X, pch = 20, ylab=expression(X[2]),xlab=expression(X[1]),
ylim = c(-4,4),xlim = c(-4,4),bty='n', col = c(rep('red',50),rep('blue',50)))
################################################################################
kNN.class = knn(train = X , test = X, cl = Y, k = 10, prob = TRUE)
kNN.class[1:100]
attr(kNN.class,"prob")
# Matriz de confusión:
mconf = table(kNN.class[1:100], Y)
print(mconf)
kNN.class[1:100]
attr(kNN.class,"prob")
# Matriz de confusión:
mconf = table(kNN.class[1:100], Y)
print(mconf)
print(mconf)
print(mconf)
print(mconf)
print(mconf)
