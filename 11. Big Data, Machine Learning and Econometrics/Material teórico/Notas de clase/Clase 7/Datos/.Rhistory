index.val   = which(folds==j) # selecciono observaciones que hacen de validación.
train = matrix(datos[ index.train,], ncol = 2)
val   = matrix(datos[ index.val  ,], ncol = 2) # mirar esquema de la diapo 30.
kNNreg = knn.reg(train = as.matrix(train[,1]) ,
y = as.matrix(train[,2]),
test = as.matrix(val[,1]),
k = k.par[i]) # El primer bucle for va cambiando los valores de "k".
mse.b[j] = sum( (kNNreg$pred - val[,2])^2 ) / 40
}
MSE[i] = mean(mse.b);
se[i]  = sd(mse.b);
print(i)
}
cbind(k.par, MSE, se)
k.par[which(MSE == min(MSE))] # k = 10
points(k.par,MSE ,type='b', col = 'blue', lwd = 2, pch = 20) #
################# kNN en R: https://cran.r-project.org/web/packages/FNN/FNN.pdf
# install.packages('FNN');
library(FNN)
###############################################################################
##### 1) Simulando los datos de entrenamiento:
f <- function(X){sin(2*X) + sqrt(X)}
X = seq(0,4*pi,length.out = 200)
set.seed(1) # Fijamos la semilla para que todos tengamos el mismo data set.
Y =  f(X) + rnorm(200)
par(mar=c(4,4,1,1))
plot(X,f(X), type = 'l',ylab='f(X)',
ylim = c(-3,6),bty='n', lwd = 3, lty = 1)
points(X,Y, pch = 20)
####################################################################
### Validation set approach ########################################
####################################################################
datos = cbind(X,Y)
set.seed(1)
id.train = sample(200,100, replace = FALSE) # que es id.train?
train = datos[ id.train, ] # <- indica que filas de Train selecciono para train.
dim(train)
head(train)
val  = datos[-id.train,]
dim(val)
k.par = c(1,5,10,20,50,80)
MSE = c() # Aquí guardo el error cuadrático medio fuera de la muestra.
for(i in 1:6){
kNNreg = knn.reg(train = as.matrix(train[,1]) ,
y = as.matrix(train[,2]),
test = as.matrix(val[,1]),
k = k.par[i])
pred = kNNreg$pred
MSE[i] = sum( (pred - val[,2])^2 ) / 100
print(i)
}
cbind(k.par, MSE)
which(MSE == min(MSE))
k.par[which(MSE == min(MSE))] # k-óptimo estimado por VC: Val-Set app.
plot(k.par, MSE, type = 'b', xlab = 'k', ylab = 'Estimación MSE',
ylim=c(0.8,2), lwd = 2, pch = 20)
# El valor estimado como adecuado para $k$ es 10.
################ End Validation-Set app.
####################################################################
### LOOCV                   ########################################
####################################################################
MSE = c() # Aquí guardo el error cuadrático medio fuera de la muestra.
sd = c()
for(i in 1:6){
mse = c() # Aquí guardo el ecm sobre cada muestra de validación.
for(j in 1:200){
train = matrix(datos[ -j,], ncol = 2)
val   = matrix(datos[  j,], ncol = 2)
kNNreg = knn.reg(train = as.matrix(train[,1]) ,
y = as.matrix(train[,2]),
test = as.matrix(val[,1]),
k = k.par[i])
mse[j] = (kNNreg$pred - val[,2])^2
} # end for  en "j"
MSE[i] = mean(mse)
sd[i] = sd(mse)
print(i)} # end for en "i"
cbind(k.par, MSE, sd)
k.par[which(MSE == min(MSE))] # k = 10
points(k.par, MSE, type = 'b', col = 'red', lwd = 2, pch = 20)
# El valor estimado como adecuado para $k$ es 10.
################ End LOOCV.
####################################################################
### B-fold CV               ########################################
####################################################################
n = 200; B = 5
folds <- cut(seq(1,n),breaks=B,labels=FALSE)
folds # Indicador de cada uno de los folds
set.seed(321) # Permuto las observaciones en cada fold.
folds = folds[sample(200,200)]
folds # table(folds)
MSE = c()  # Aquí guardo el error cuadrático medio para cada valor de k.
se  = c()  # Aquí guardo el error estandard para cada valor de k.
for(i in 1:6){ # i recorre los valores del parámetro "k" (ver "k.par[i]" más abajo).
mse.b = c()      # Vector auxiliar en donde iré guardando el ecm para cada valor de k en cada fold de validación.
for(j in 1:B){ # j-recorre los folds
index.train = which(folds!=j) # selecciono observaciones que hacen de train.
index.val   = which(folds==j) # selecciono observaciones que hacen de validación.
train = matrix(datos[ index.train,], ncol = 2)
val   = matrix(datos[ index.val  ,], ncol = 2) # mirar esquema de la diapo 30.
kNNreg = knn.reg(train = as.matrix(train[,1]) ,
y = as.matrix(train[,2]),
test = as.matrix(val[,1]),
k = k.par[i]) # El primer bucle for va cambiando los valores de "k".
mse.b[j] = sum( (kNNreg$pred - val[,2])^2 ) / 40
}
MSE[i] = mean(mse.b);
se[i]  = sd(mse.b);
print(i)
}
cbind(k.par, MSE, se)
k.par[which(MSE == min(MSE))] # k = 10
points(k.par,MSE ,type='b', col = 'blue', lwd = 2, pch = 20) #
################# kNN en R: https://cran.r-project.org/web/packages/FNN/FNN.pdf
# install.packages('FNN');
library(FNN)
###############################################################################
##### 1) Simulando los datos de entrenamiento:
f <- function(X){sin(2*X) + sqrt(X)}
X = seq(0,4*pi,length.out = 200)
set.seed(1) # Fijamos la semilla para que todos tengamos el mismo data set.
Y =  f(X) + rnorm(200)
par(mar=c(4,4,1,1))
plot(X,f(X), type = 'l',ylab='f(X)',
ylim = c(-3,6),bty='n', lwd = 3, lty = 1)
points(X,Y, pch = 20)
####################################################################
### Validation set approach ########################################
####################################################################
datos = cbind(X,Y)
set.seed(1)
id.train = sample(200,100, replace = FALSE) # que es id.train?
train = datos[ id.train, ] # <- indica que filas de Train selecciono para train.
dim(train)
head(train)
val  = datos[-id.train,]
dim(val)
k.par = c(1,5,10,20,50,80)
MSE = c() # Aquí guardo el error cuadrático medio fuera de la muestra.
for(i in 1:6){
kNNreg = knn.reg(train = as.matrix(train[,1]) ,
y = as.matrix(train[,2]),
test = as.matrix(val[,1]),
k = k.par[i])
pred = kNNreg$pred
MSE[i] = sum( (pred - val[,2])^2 ) / 100
print(i)
}
cbind(k.par, MSE)
which(MSE == min(MSE))
k.par[which(MSE == min(MSE))] # k-óptimo estimado por VC: Val-Set app.
plot(k.par, MSE, type = 'b', xlab = 'k', ylab = 'Estimación MSE',
ylim=c(0.8,2), lwd = 2, pch = 20)
# El valor estimado como adecuado para $k$ es 10.
################ End Validation-Set app.
####################################################################
### LOOCV                   ########################################
####################################################################
MSE = c() # Aquí guardo el error cuadrático medio fuera de la muestra.
sd = c()
for(i in 1:6){
mse = c() # Aquí guardo el ecm sobre cada muestra de validación.
for(j in 1:200){
train = matrix(datos[ -j,], ncol = 2)
val   = matrix(datos[  j,], ncol = 2)
kNNreg = knn.reg(train = as.matrix(train[,1]) ,
y = as.matrix(train[,2]),
test = as.matrix(val[,1]),
k = k.par[i])
mse[j] = (kNNreg$pred - val[,2])^2
} # end for  en "j"
MSE[i] = mean(mse)
sd[i] = sd(mse)
print(i)} # end for en "i"
cbind(k.par, MSE, sd)
k.par[which(MSE == min(MSE))] # k = 10
points(k.par, MSE, type = 'b', col = 'red', lwd = 2, pch = 20)
# El valor estimado como adecuado para $k$ es 10.
################ End LOOCV.
####################################################################
### B-fold CV               ########################################
####################################################################
n = 200; B = 5
folds <- cut(seq(1,n),breaks=B,labels=FALSE)
folds # Indicador de cada uno de los folds
set.seed(321) # Permuto las observaciones en cada fold.
folds = folds[sample(200,200)]
folds # table(folds)
MSE = c()  # Aquí guardo el error cuadrático medio para cada valor de k.
se  = c()  # Aquí guardo el error estandard para cada valor de k.
for(i in 1:6){ # i recorre los valores del parámetro "k" (ver "k.par[i]" más abajo).
mse.b = c()      # Vector auxiliar en donde iré guardando el ecm para cada valor de k en cada fold de validación.
for(j in 1:B){ # j-recorre los folds
index.train = which(folds!=j) # selecciono observaciones que hacen de train.
index.val   = which(folds==j) # selecciono observaciones que hacen de validación.
train = matrix(datos[ index.train,], ncol = 2)
val   = matrix(datos[ index.val  ,], ncol = 2) # mirar esquema de la diapo 30.
kNNreg = knn.reg(train = as.matrix(train[,1]) ,
y = as.matrix(train[,2]),
test = as.matrix(val[,1]),
k = k.par[i]) # El primer bucle for va cambiando los valores de "k".
mse.b[j] = sum( (kNNreg$pred - val[,2])^2 ) / 40
}
MSE[i] = mean(mse.b);
se[i]  = sd(mse.b);
print(i)
}
cbind(k.par, MSE, se)
k.par[which(MSE == min(MSE))] # k = 10
points(k.par,MSE ,type='b', col = 'blue', lwd = 2, pch = 20) #
################# kNN en R: https://cran.r-project.org/web/packages/FNN/FNN.pdf
# install.packages('FNN');
library(FNN)
czc
rm(list=ls())
#------------------ Bagging: Caret + Rpart ---------------------#
require(caret) || install.packages('caret'); library(caret) #
require(rpart) || install.packages('rpart'); library(rpart) #
setwd('G:/Mi unidad/JM/Facultad de Ciencias Económicas (FCE)/Maestría en Econometría/11. Big Data, Machine Learning and Econometrics/Material teórico/Notas de clase/Clase 7/Datos/')
datos = read.table(file = 'encuesta.txt', sep = ',', dec = '.', header = T)
head(datos, 3) # Ver diccionario de variables en campus.
dim(datos)
sum(complete.cases(datos)) # No hay datos perdidos (fueron imputados con la libreria MissingForest).
### Objetivo:
table(datos$Party)/5568 # 53% Democrats vs 47% Rep.
datos$Party = as.factor(datos$Party)
set.seed(321)
id.train = sample(5568, 4568)
set.seed(1)
f.bagg<- train(Party ~ .,
data = datos[id.train,],
weights = NULL,# Modelos robustos a datos atípicos.
method = "treebag",# Ensamblamos árboles.
trControl = trainControl(method = "oob"),
keepX= T , # Estimación del error por oob
metric = 'Accuracy',
nbagg = 100, # B (Lo más grande que puedas)
control = rpart.control(minsplit = 800)) # Parámetros de compljidad de los árboles.
print(f.bagg) #Acc = 0.67 -> Error = 33% (approx).
1- f.bagg$results[1]
varImp(f.bagg) # Importancia de cada variable en el ensamble.
x11(); dotPlot(varImp(f.bagg)) # Idem pero en una gráfica.
# Predicciones sobre TEST (no cross-validamos la complejidad del modelo)
pred.bagg = predict(f.bagg, newdata = datos[-id.train,])
head(pred.bagg, 3)
table(pred.bagg, datos$Party[-id.train])/1000
# Tarea en casa: Optimiza el hiperparámetro minsplit utilizando OOB.
valores.minsplit = c(500, 600, 700, 900)
hat.te.oob = c()
for(i in 1:4){
f.bagg<- train(Party ~ .,
data = datos[id.train,],
method = "treebag",
trControl = trainControl(method = "oob"),
keepX= T ,
metric = 'Accuracy',
nbagg = 100,
control = rpart.control(minsplit = valores.minsplit[i]))
hat.te.oob[i] = 1- f.bagg$results[1]
print(i)
}
set.seed(1)
f.bagg<- train(Party ~ .,
data = datos[id.train,],
method = "treebag",
trControl = trainControl(method = "oob"),
keepX= T ,
metric = 'Accuracy',
nbagg = 100,
control = rpart.control(minsplit = 600))
pred.bagg = predict(f.bagg, newdata = datos[-id.train,])
table(pred.bagg, datos$Party[-id.train])/1000 # acc = 33% aprox.
rm(list=ls())
#------------------ Bagging: Caret + Rpart ---------------------#
require(caret) || install.packages('caret'); library(caret) #
require(rpart) || install.packages('rpart'); library(rpart) #
setwd('G:/Mi unidad/JM/Facultad de Ciencias Económicas (FCE)/Maestría en Econometría/11. Big Data, Machine Learning and Econometrics/Material teórico/Notas de clase/Clase 7/Datos')
datos = read.table(file = 'encuesta.txt', sep = ',', dec = '.', header = T)
rm(list=ls())
require(randomForest) || install.packages('randomForest')#
setwd('G:/Mi unidad/JM/Facultad de Ciencias Económicas (FCE)/Maestría en Econometría/11. Big Data, Machine Learning and Econometrics/Material teórico/Notas de clase/Clase 7/Datos')
datos = read.table(file = 'encuesta.txt', sep = ',', dec = '.', header = T)
head(datos, 3) # Ver diccionario de variables en campus.
datos$Party = as.factor(datos$Party) # formato RandomForest para clasificación.
set.seed(321)
id.train = sample(5568, 4568)
# ?randomForest
forest.bench = randomForest(Party~.,
data=datos[id.train,],
mtry =10, # m = 10 (si m = cantidad features entonces estamos haciendo Bagging)
ntree=500,# B
sample = 3000, # tamaño de cada re-muestra bootstrap.
maxnodes = 10,# cantidad máxima de nodos terminales en c/ árbol.
nodesize = 150, # cantidad mínima de datos en nodo terminal.
importance=T, # Computar importancia de c/ covariable.
# proximity = T,#computa la matriz de proximidad entre observaciones.
# na.action =na.roughfix # imputa perdidos con mediana / moda.
# na.action = rfImpute # imputa perdidos con datos próximos.
# na.action = na.omit# descarta valores perdidos.
)
#@ Respecto del Fitting del modelo:
head(forest.bench$oob.times,3) # nro de veces q c/obs quedó out-of-bag.
head(forest.bench$votes,3) # (OOB)
head(forest.bench$predicted,3) # (ídem pero catagorías).
forest.bench$confusion # Matriz de confusión (OOB)
# Performance: (estimaciones OOB)
forest.bench # (888 + 610)/4586 = 32.8% (un poquito mejor que Bagging)
# Importancia de cada variable: (+ alto = más contribución).
# www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#varimp
X11()
varImpPlot(forest.bench, main ='Importancia de cada feature')
############### Sintonía fina de hiper-parámetros (OOB):
valores.m = c(20,50,100)# valores de "m"
valores.maxnode = c(20,50,100)# Complejidad de árboles en el bosque.
parametros = expand.grid(valores.m = valores.m,valores.maxnode = valores.maxnode)
# En la práctica exploramos una grilla mucho más grande.
head(parametros,3) # Matriz de 12 filas x 2 columnas.
te = c() # Tasa de error estimada por OOB.
set.seed(1)
for(i in 1:dim(parametros)[1]){ # i recorre la grilla de parámetros.
forest.oob= randomForest(Party~.,
data=datos[id.train,],
mtry = parametros[i,1], # m
ntree=500,# Mientras más grande, mejor.
sample = 3000,# Tamaño de cada re-muestra bootstrap
maxnodes = parametros[i,2], # complejidad de cada árbol del bosque
nodesize = 150,
proximity =F)
te[i] = 1 - sum(diag(forest.oob$confusion[,-3]))/4568
print(i)
}
cbind(parametros,te) # Estimaciones de la TE por OOB.
which(min(te)==te)
parametros[5,] # m* = 50, maxnode* = 50 (estimaciones de mala calidad porque B es relativamente pequeño)
library("scatterplot3d"); x11()
scatterplot3d(cbind(parametros,te),type = "h", color = "blue")
set.seed(123)
modelo.final = randomForest(Party~.,
data=datos[id.train,],
mtry = 50, # m*
ntree= 500,
sample = 3000,
maxnodes = 50, # complejidad*
nodesize = 150,
importance = F,
proximity= F
)
##---- FIN de selección de modelo.
#### Extrapolamos como funcionaría el modelo seleccionado con el conjunto de test:
pred.rfor.test = predict(modelo.final,newdata=datos[-id.train,])
matriz.conf = table(pred.rfor.test,datos$Party[-id.train])
matriz.conf
1-sum(diag(matriz.conf))/1000 # 31.9% ;)
# vs Modelo CART 38% vs Bagging 33%
#------------------------------------------------------------ END.
rm(list = ls()); dev.off()
# --- Pipeline para implementar R.Forest y Boosting en R ----------------------#
require(randomForest) || install.packages('randomForest');library(randomForest)#
library(randomForest)
calif = read.table("http://www.stat.cmu.edu/~cshalizi/350/hw/06/cadata.dat", header=TRUE)
str(calif) # dim(calif) # 20640 instancias, 9 variables:
#@ 2: Preprocesamiento de datos y análisis descriptivo mínimo.
set.seed(1)
id.train=sample(20640,10000)
### Precios meadino en funcion de las coord. de latitud-longitud
x11()
price.deciles = quantile(calif$MedianHouseValue,0:10/10)
cut.prices = cut(calif$MedianHouseValue,price.deciles,include.lowest=TRUE)
plot(calif$Longitude,calif$Latitude,col=grey(10:2/11)[cut.prices],pch=20,
xlab="Longitud",ylab="Latitud",main='Precios de la vivienda en California')
#@ 3: Fitting de un primer modelo Forest de 'Benchmark'
?randomForest
forest.bench = randomForest(log(MedianHouseValue)~.,
data=calif[id.train, ],
mtry=round(ncol(calif)/3), # (Slide 29) *** Si "mtry = cantidad de features dispoibles" entonces RF es en realidad Bagging***
nodesize = 5,# (Slide 29)
ntree=500,# B
importance=T) # (Slide 20)
forest.bench
x11()
barplot(sort(forest.bench$importance[,1], decreasing = TRUE),
main = "Contribución % a la reducción del MSE", col = "lightblue",
horiz = TRUE, las = 1, cex.names = .6, xlim = c(0, .25))
x11();# Prediciones vs valores observados (TRAIN, estimaciones OOB):
plot(log(calif$MedianHouseValue[id.train]),forest.bench$predicted,xlab="Y",
ylab=expression(hat(Y)),main="Pred vs. Observados",pch='.')
abline(0,1,lwd=2,col=2) # El modelo parece bastante bueno!
seq(1,100, by = 5)
valores.m = c(2,3,4,6,8) # <- Posibles valores para "m".
valores.maxnode = c(5,10,15,20,30) # <- Posibles valores para el parámetro de complejidad del modelo.
matriz.resultados = expand.grid(m = valores.m, maxnode = valores.maxnode, ecm.oob = 0)
head(matriz.resultados, 2) # <- De aquí voy a tomar los hiperparámetros y a colocar el ecm estimado por oob.
for(i in 1:dim(matriz.resultados)[1] ){ # Equivale a VC (más facil de implementar)
optim.forest = randomForest(log(MedianHouseValue)~.,
data=calif[id.train, ],
ntree=500, # En la práctica utiliza un valor de B >> 0.
mtry = matriz.resultados[i,1], # <- Posibles valores de 'm'
maxnodes = matriz.resultados[i,2]) # <- Posibles valores de 'complexity parameter'
matriz.resultados[i,3] <-sum( (log(calif$MedianHouseValue[id.train]) - optim.forest$predicted)^2 ) / 10640 # m?trica es el ECM estimado por OOB
print(i)
}
### Resultados:
matriz.resultados # m* = 6 y complexity* = 30
set.seed(1)
forest.best = randomForest(log(MedianHouseValue)~.,
data=calif[id.train, ],
mtry=6,# m*
maxnodes = 30, # complexity*
ntree=500)
pred.for.test = predict(forest.best,newdata=calif[-id.train, ]) # Predicciones test.
MSE.for = sum((pred.for.test - log(calif$MedianHouseValue[-id.train]))^2)/10000
MSE.for #### 0.1148 (benchmark para contrastar contra 'gbm').
################################################################################
require(gbm) || install.packages('gbm') ; library(gbm)##################
################################################################################
set.seed(1)
bos.gbm = gbm(log(MedianHouseValue)~.,
data=calif[id.train, ],
distribution="gaussian",# Regresión (Y continua). En clasificación: distribution="bernulli"
n.trees=5000, # Tamaño de la secuencia (M en slide 44)
shrinkage=.1, # Parámetro lambda en slide 44.
train.fraction=0.8, # Parámetro 'delta' en slide 44.
bag.fraction=0.7, # Parámetro 'eta' en slide 44.
interaction.depth=3,# Parámetro de complejidad de cada modelo en la secuencia (utilizar modelos simples)
cv.folds=5, # Si cv.folds > 0 estima también por VC (train frac) el error del modelo (puede ponerse bastante lento el entrenamiento)
keep.data=T,
verbose=T)
x11() # Análisis del ensamblaje vía Boosting
gbm.perf(bos.gbm,method="cv")
title(main="ECM estimado por VC, en Validación y en Training")
legend(500,0.3,c("Error 5CV","Error Validación","Error en Train"),
col=c('green','red','black'),pch='-') # Hay que truncar el ensamble para evitar overfitting!
M.star = gbm.perf(bos.gbm,method="cv");
M.star # Truncamos el ensamble.
summary(bos.gbm,n.trees=M.star) # <- Importancia de cada variable respecto del modelo seleccionado.
m.prec.gmb = predict(bos.gbm,n.trees=M.star)
x11()
plot(log(calif$MedianHouseValue[id.train]),m.prec.gmb,xlab="Y",
ylab=expression(hat(Y)),main="Predicciones vs. Valores observados",pch='.')
abline(0,1,lwd=2,col=2)
### Resultados sobre test:
pred.gbm.test=predict(bos.gbm,newdata=calif[-id.train,],n.trees=M.star)
MSE.gbm = sum((pred.gbm.test - log(calif$MedianHouseValue[-id.train]))^2)/10000
MSE.gbm# 0.058 vs 0.1148 (Random Forest).
###############################################################################
rm(list=ls())
##########################################################
require(xgboost) || install.packages('xgboost') #####
##########################################################
library(xgboost)
setwd('G:/Mi unidad/JM/Facultad de Ciencias Económicas (FCE)/Maestría en Econometría/11. Big Data, Machine Learning and Econometrics/Material teórico/Notas de clase/Clase 7/Datos')
load('colisiones.Rdata')
dim(colisiones)
head(colisiones, 2)
table(colisiones$VAR_OBJETIVO) # columna 1: Y = 0 (sin colisión declarada último año, Y = 1, una o más colisiones)
table(colisiones$VAR_OBJETIVO)/dim(colisiones)[1]*100
table(colisiones$FREC_COLISION_ANYO_0) # Quitar columna 19 del data set antes de empezar a modelar.
colisiones = colisiones[,-19]
###1] Ingeniería de atributos:
colisiones$pp = colisiones$POTENCIA /colisiones$PESO
set.seed(1)
id.train=sample(dim(colisiones)[1],round(dim(colisiones)[1]*0.5))
# xgboost trabaja con matrices de datos sparse.
xgb_train = xgb.DMatrix(data = data.matrix(colisiones[ id.train,-1]), label = data.matrix(colisiones[ id.train,1]))
xgb_test= xgb.DMatrix(data = data.matrix(colisiones[-id.train,-1]), label = data.matrix(colisiones[-id.train,1]))
watchlist <- list(train=xgb_train, test=xgb_test)
set.seed(1)
benchmark <- xgb.train(data = xgb_train,
watchlist=watchlist,# Estimamos la devianza sobre train y test.
nrounds = 1000, # Parámetro M
early_stopping_rounds = 50, # El algoritmo corta si después de 50 iteraciones no hay mejora sobre el conjunto de test.
print_every_n = 50,
param = list(
max.depth = 3,# Complejidad de cada modelo.
eta = 0.05,# Lambda en slide 44.
subsample = 0.75), # eta en slides 44.
# Para más opciones explorar la ayuda de esta función.
nthread = 8, # Número de núcleos.
objective = "binary:logistic" # Función de pérdida (Deviance, ver apéndice en slides).
)
benchmark # Hay que cross--validar los parámetros.
importance_matrix <- xgb.importance(model = benchmark)
head(importance_matrix, 3)
x11()
xgb.plot.importance(importance_matrix = importance_matrix)
pred <- predict(benchmark, newdata = xgb_test)
head(pred, 3) # utilizamos la función g(x)=e^x/(1+e^x) que transforma el output del ensamble en 'probabilidades'
y.test = ifelse(pred >0.3,1,0)
head(y.test, 3)
table(y.test, colisiones$VAR_OBJETIVO[-id.train])/171214*100
# Apéndice (sobre 1)
########################################################
#2: Selección de modelos con Boosting (xgboost): ###
########################################################
param <- expand.grid(
max_depth = c(2,4,6),
eta = c(0.05, 0.1)
# Otros hyperparámetros: https://xgboost.readthedocs.io/en/latest/parameter.html
)
print(param) # En la práctica deberías explorar grillas más extensas.
auc = M.star = c()
set.seed(1)
optim.boost <- xgb.train(data = xgb_train,
nrounds = 1000,
nthread = 8,
watchlist=watchlist,
objective = "binary:logistic",
params = list(
max.depth = 4,
eta = 0.05,
subsample = 0.5)
)
