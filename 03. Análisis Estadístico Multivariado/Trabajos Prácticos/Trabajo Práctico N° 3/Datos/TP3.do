/*---------------------------------------------------------------------------*
 |
 |            				    PS7:AEM, 2021
 |
 *--------------------------------------------------------------------------*/

/*
Cluster analysis attempts to determine the natural groupings (or clusters) of observations. Sometimes
this process is called “classification”, but this term is used by others to mean discriminant analysis,
which is related but is not the same. To avoid confusion, usually we use “cluster
analysis” or “clustering” when referring to finding groups in data.

It is "exploratory", and that explains the largely absent ‘p-value’. Clustering 
methods are intended largely for generating rather than testing hypotheses
*/ 
 
/*---------------------------------------------------------------------------*
 |                                                                         
 |            			Problem 1: Inicializando Stata							   
 |																		   
 *--------------------------------------------------------------------------*/ 

version 11
clear
set more off

use firmas.dta, clear
rename ebitass ebitda

foreach var in ebitda rotc {  
	quietly sum `var'
	replace `var' = (`var'-r(mean))/r(sd) //	standardization of each variable
                             }
//

/*---------------------------------------------------------------------------*
 |                                                                         
 |            						Problem 1: a)					   
 |																		   
 *--------------------------------------------------------------------------*/ 

do kmedtest.do  /*F test de reducción de variabilidad: testea si crear un nuevo grupo genera la suficiente reducción de variabilidad*/

//	H0: the cluster solutions with g1 and g2 clusters are equally valid
//	H1: the cluster solution with g2 clusters is better than the solution with g1 clusters (g2 > g1).
kmedtest ebitda rotc, k(1)
kmedtest ebitda rotc, k(2)
kmedtest ebitda rotc, k(3)
kmedtest ebitda rotc, k(4)
kmedtest ebitda rotc, k(5)
//	Empirical rule, add one more group if the statistic is greater than 10: you should choose two clusters

scatter ebitda rotc

twoway (scatter ebitda rotc if group == 0, legend(label(1 "Group 0"))) ///
(scatter ebitda rotc if group == 1, legend(label(2 "Group 1")))

graph matrix ebitda rotc
//	The data indicate a wide range of levels of performance for the firms. The 
//	graph seems to indicate that there are some distinct groups.

/*
Kmeans and kmedians clustering are iterative procedures that partition the data into k groups or
clusters. The procedure begins with k initial group centers. Observations are assigned to the group
with the closest center. The mean or median of the observations assigned to each of the groups is
computed, and the process is repeated. These steps continue until all observations remain in the same
group from the previous iteration.
*/

//	4 groups
cluster kmeans ebitda rotc, k(4) generate(predict) 
//	K-means method with 4 means taken randomly. "predict" is the name of the 
//	grouping variable.

//	2 groups
cluster kmeans ebitda rotc, k(2) generate(predict2)

/*---------------------------------------------------------------------------*
 |                                                                         
 |            						Problem 1: b)					   
 |																		   
 *--------------------------------------------------------------------------*/ 
 
discrim lda ebitda rotc, group(group) 
predict ngroup, classification	//	creates the variable "group" where it 
//	assigns the prediction based on the classification rule created
//	Both models (k = 2 and discriminant) group in the same way

//	We will create a graph of points where each color represents one of the 4 
//	groups that were generated by the k-means process
twoway (scatter ebitda rotc if predict==1,legend(label(1 "Grupo1")) mcolor("255 128 64")) 	///
(scatter ebitda rotc if predict==2, legend(label(2 "Grupo2")) mcolor("0 0 255")) 			///
(scatter ebitda rotc if predict==3, legend(label(3 "Grupo3"))  mcolor("255 255 0")) 		///
(scatter ebitda rotc if predict==4, legend(label(4 "Grupo4")) mcolor("100 100 0"))

//	Discrim vs cluster
twoway (scatter ebitda rotc if predict==1, msymbol(circle_hollow) msize(large) legend(label(1 "Grupo1")) mcolor("255 128 64")) ///
(scatter ebitda rotc if predict==2, msymbol(circle_hollow) msize(large) legend(label(2 "Grupo2")) mcolor("128 255 128")) ///
(scatter ebitda rotc if predict==3, msymbol(circle_hollow) msize(large) legend(label(3 "Grupo3")) mcolor("255 255 0")) ///
(scatter ebitda rotc if predict==4, msymbol(circle_hollow) msize(large) legend(label(4 "Grupo4")) mcolor("100 100 0")) 

twoway (scatter ebitda rotc if predict==1, msymbol(circle_hollow) msize(large) legend(label(1 "Grupo1")) mcolor("255 128 64")) ///
(scatter ebitda rotc if predict==2, msymbol(circle_hollow) msize(large) legend(label(2 "Grupo2")) mcolor("128 255 128")) ///
(scatter ebitda rotc if predict==3, msymbol(circle_hollow) msize(large) legend(label(3 "Grupo3")) mcolor("255 255 0")) ///
(scatter ebitda rotc if predict==4, msymbol(circle_hollow) msize(large) legend(label(4 "Grupo4")) mcolor("100 100 0")) ///
(scatter ebitda rotc if ngroup==0, legend(label(5 "Good(Discrim)")) msymbol(triangle) msize(small) mcolor("255 0 0")) ///
(scatter ebitda rotc if ngroup==1, legend(label(6 "Bad(Discrim)")) msymbol(triangle) msize(small) mcolor("0 0 255"))


/*---------------------------------------------------------------------------*
 |                                                                         
 |            						Problem 1: c)					   
 |																		   
 *--------------------------------------------------------------------------*/
  
//	Con el dendrogram se presenta la información sobre qué observaciones se 
//	agrupan en varios niveles de similitud. Las líneas verticales se extienden 
//	hacia arriba para cada observación, y en varios valores de similitud, estas 
//	líneas están conectadas a las de otras observaciones con una horizontal. 
//	Las observaciones continúan combinándose hasta que, en la parte superior 
//	del dendrograma, todas se agrupan.
//	La altura de las líneas verticales y el rango del eje de similitud dan 
//	pistas visuales sobre la fuerza de la agrupación. Las verticales largas 
//	indican una separación más clara entre los grupos. Las más cortas indican 
//	grupos que no son tan distintos.
//	En este caso, las líneas verticales representan a los países. Cuando las 
//	uniones se encuentran más abajo en el árbol, indica que las observaciones 
//	son similares entre sí, mientras que, si las uniones se dan más arriba, las 
//	observaciones son disímiles. Por lo tanto, la altura a la que se produce la 
//	unión nos indicará que tan distintos son, lo cual se puede interpretar como 
//	variabilidad dentro de cada grupo. El análisis horizontal nos dará noción 
//	también sobre qué tan distintos son los conjuntos, lo cual nos indica 
//	variabilidad entre grupos.

cluster singlelinkage ebitda rotc, name(aux2) generate(predict2)
//	Things we saved: 
//		- 	name(...) specifies the name of the cluster object. That is the 
//			object you call when doing the dendrogram
//		-	predict2_id is just the name of the observation
//		-	predict2_ord gives the order from left to right in the dendrogram,
//			just to know where to look when you want to know where an 
//			observation is
//		-	predict3_hgt is the distance between the observation and the cluster
//			with which it merged

cluster stop aux2   	//	Gives you the Calinski Harabasz pseudof F stopping rule    
clustermat stop aux2	//	Gives you the Duda-Hart stopping rule
cluster dendrogram aux2
//	The following variable indicates which observation belong to each group 
//	(considering 2 groups, that is, the last two groups to merge). A clustering 
//	of the data objects is obtained by cutting the dendrogram at the desired 
//	level, then each connected component forms a cluster
cluster generate aux2bis = group(2), name(aux2) 	
twoway (scatter ebitda rotc if aux2==1,legend(label(1 "Grupo1")) mcolor("255 128 64")) (scatter ebitda rotc if aux2==2, legend(label(2 "Grupo2")) mcolor("0 0 255"))

//	Complete linkage 
cluster completelinkage ebitda rotc, name(aux3) generate(predict3)
cluster stop aux3        
cluster dendrogram aux3
cluster generate aux3 = group(2), name(aux3)
twoway (scatter ebitda rotc if aux3==1,legend(label(1 "Grupo1")) mcolor("255 128 64")) (scatter ebitda rotc if aux3==2, legend(label(2 "Grupo2")) mcolor("0 0 255"))

//	Centroid linkage (Centroid linkage merges the groups whose means are closest)
cluster centroidlinkage ebitda rotc, name(aux4) generate(predict4)
cluster stop aux4        
cluster dendrogram aux4
cluster generate aux4 = group(2), name(aux4)
twoway (scatter ebitda rotc if aux4==1,legend(label(1 "Grupo1")) mcolor("255 128 64")) (scatter ebitda rotc if aux4==2, legend(label(2 "Grupo2")) mcolor("0 0 255"))


//	Ward linkage (Ward's minimum variance criterion minimizes the total 
//	within-cluster variance. To implement this method, at each step find the 
//	pair of clusters that leads to minimum increase in total within-cluster 
//	variance after merging. 
cluster wardslinkage ebitda rotc, name(aux6) generate(predict6)
cluster stop aux6       
cluster dendrogram aux6
cluster generate aux6 = group(2), name(aux6)
twoway (scatter ebitda rotc if aux6==1,legend(label(1 "Grupo1")) mcolor("255 128 64")) (scatter ebitda rotc if aux6==2, legend(label(2 "Grupo2")) mcolor("0 0 255"))

//	Notice all the dendrograms start with the same merging.
//	Notice that the vertical axis is in different scales.